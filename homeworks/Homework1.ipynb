{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1) \\nMachine learning is a subset of artificial intelligence. It is the concept that computers can learn from existing data sets and generate new predictions.\\nWhat's more, it can do all these staff without human intervention. Because it can associate and learn data just like a human.\\n\\n2) \\nIn supervised learning, there are input and output variables and an appropriate algorithm is used to learn the way from input to the output.\\nThere are correct answers in this type of learning and the predictions are done to reach the correct results.\\nThe data can be evaluated as classification and regression.\\n\\nExample algorithms for supervised learning: \\n1- K-Nearest Neighbors (KNN)\\n2- Linear regression \\n3- logistic regression  \\n\\nIn unsupervised learning, there are input variables, but not the outputs.\\nUnlike supervised learning, in unsupervised learning, there are no correct results. It is used to model distribution in the data and it learns by itself.\\nThe data can be evaluated as clustering and association.\\n\\nExample algorithms for unsupervised learning: \\n1- Apriori\\n2- K-Means\\n3- Principle Component Analysis (PCA)\\n\\n3)\\nTest Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\\nIt is only used once a model is completely trained.\\n\\nValidation Dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.\\nThe validation set is used to evaluate a given model, but this is for frequent evaluation. It can be called development stage of the data.\\n\\n4) \\nWith data preparation, the quality of the data set and the efficiency for the result can be increased by\\nclearing the available data from unnecessary information and making estimates for missing information.\\n\\nExplarotary Data Analysis (EDA) provides to perform initial investigations on data so as to discover patterns,\\nto spot anomalies, to test hypothesis and to check assumptions with some methods.\\n1- Preprocessing is a very important step to ensure the quality of the owned data.\\n2- Duplicate Values: There can be some duplicate values in every datasets and needed to be removed from it to increase the quality of the dataset.\\n3- Imbalanced Data: Imbalanced classification refers to a classification predictive modeling problem where the number of examples in the training dataset for each class label is not balanced.\\n4- Missing Values: In every dataset, there could be missing values at any points.\\n\\nIn order to handle with this problem, there are two common method: \\n1- Eliminating the missing value.\\n2- Filling it with mean or medium.\\n\\nOutlier Detection: Outliers are data points that do not belong to a certain population.\\n\\nIt is an abnormal observation that lies far away from other values. Briefly, an outlier is an observation that diverges from otherwise well-structured data.\\nThere are five common methods to deal with it:\\n1-Standard Deviation: If there are any data points that are more than 3 times the standard deviation, then those points are very likely to be anomalous or outliers.\\n\\n2-Boxplots: Box plots are a graphical depiction of numerical data through their quantiles. Any data points that show above or below the whiskers, can be considered outliers or anomalous.\\n3- DBScan Clustering: DBScan is a clustering algorithm used cluster data into groups. It is also used as a density-based anomaly detection method with either single or multi-dimensional data.\\nIt is a more complicated method than standard deviation and boxplots.\\n\\n4- Isolation Forest: Isolation Forest is an unsupervised learning algorithm that belongs to the ensemble decision trees family. It is a different method from others.\\nIt explicitly isolates anomalies instead of profiling and constructing normal points and regions by assigning a score to each data point.\\n\\n5- Robust Random Cut Forest: Random Cut Forest (RCF) algorithm is Amazon’s unsupervised algorithm for detecting anomalies.\\nIt works by associating an anomaly score as well. Low score values indicate that the data point is considered as normal. High values indicate the presence of an anomaly in the data.\\n\\nFeature Scaling: There are two types of scaling the features: \\n1- Standardization \\n2- Normalization.\\n\\nStandardization: The new value is defined as dependent on the standard deviation and the mode of the data.\\n\\nNormalization: The new value is dependent on:\\n1- when the feature is more or less uniformly distributed across a fixed range: linear scaling -> maximum and minimum values of data inputs\\n2- when the feature contains extreme outliers: clipping -> maximum and minimum values of data inputs\\n3- when the feature conforms to the power law: log scaling -> logarithms of data inputs\\n4- when the feature distribution does not contain extreme values: z-score -> the standard deviation and the mode of the data\\n    \\nBucketing (Binning): It is an important method used to minimize the effects of noisy data.The actual data values are divided into small intervals known as bins and then they are replaced by a general value calculated for that bin.\\n\\nFeature Encoding: Some transformations can be performed on the data so that it can be easily accepted as input for machine learning algorithms.\\nThere are two common types of feature encoding:\\n1- Nominal: Any one-to-one mapping can be done which retains the meaning. Example: One-hot encoding -> 001: Red 010: Yellow 100: Green for a traffic light encoding.\\n2- Ordinal: An order-preserving change of values. Example: 0: less 1: equal 2: more\\n    \\nTrain/Validation/Test Split: There are the explanation of types of dataset above. The split process is very important in data preparation.\\nAvailable data refers to training dataset and test dataset while new available data refers to training dataset and validation dataset.\\nIt is more accurate to use validation dataset together with others. The percentage of training dataset should be more than the other types.\\n\\nCross Validation: Testing a prediction function on the same data is a critical mistake. It is called overfitting.\\nTo avoid it, the most effective way is to hold out part of the available data as a test set.\\n\\n5)\\nDiscrete data involves round, concrete numbers that are determined by counting while continuous data involves complex numbers that are measured across a specific time interval.\\nThat can be said discrete variables are countable and finite but continuous variables are uncountable and infinite (take an infinite time to count).\\nExample of discrete variable: The number of students in a school (as points)\\nExample of continuous variable: The annual temperature values (as a distinct line)\\n    \\n6)\\nThis is a graph showing flower width on the horizontal axis and their proportions on the vertical axis.\\nThis graph consist of a curve (distinct line) so the type of data variable is continuous. Rises and drops are observed according to the rate of change of the distribution.\\n\\nProcess:\\n    1- Gathering Data  \\n    2- Sorting into Balanced and Imbalanced Data  \\n    3- Balancing the Imbalanced Data  \\n    4- Fitting Missing Values with the Mean or Median \\n    5- Cleaning Noise Data\\n    6- Standardization and Normalization \\n    7- Feature Extraction (PCA?) \\n    8- Dataset Split \\n    9- Ready\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1) \n",
    "Machine learning is a subset of artificial intelligence. It is the concept that computers can learn from existing data sets and generate new predictions.\n",
    "What's more, it can do all these staff without human intervention. Because it can associate and learn data just like a human.\n",
    "\n",
    "2) \n",
    "In supervised learning, there are input and output variables and an appropriate algorithm is used to learn the way from input to the output.\n",
    "There are correct answers in this type of learning and the predictions are done to reach the correct results.\n",
    "The data can be evaluated as classification and regression.\n",
    "\n",
    "Example algorithms for supervised learning: \n",
    "1- K-Nearest Neighbors (KNN)\n",
    "2- Linear regression \n",
    "3- logistic regression  \n",
    "\n",
    "In unsupervised learning, there are input variables, but not the outputs.\n",
    "Unlike supervised learning, in unsupervised learning, there are no correct results. It is used to model distribution in the data and it learns by itself.\n",
    "The data can be evaluated as clustering and association.\n",
    "\n",
    "Example algorithms for unsupervised learning: \n",
    "1- Apriori\n",
    "2- K-Means\n",
    "3- Principle Component Analysis (PCA)\n",
    "\n",
    "3)\n",
    "Test Dataset: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "It is only used once a model is completely trained.\n",
    "\n",
    "Validation Dataset: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.\n",
    "The validation set is used to evaluate a given model, but this is for frequent evaluation. It can be called development stage of the data.\n",
    "\n",
    "4) \n",
    "With data preparation, the quality of the data set and the efficiency for the result can be increased by\n",
    "clearing the available data from unnecessary information and making estimates for missing information.\n",
    "\n",
    "Explarotary Data Analysis (EDA) provides to perform initial investigations on data so as to discover patterns,\n",
    "to spot anomalies, to test hypothesis and to check assumptions with some methods.\n",
    "1- Preprocessing is a very important step to ensure the quality of the owned data.\n",
    "2- Duplicate Values: There can be some duplicate values in every datasets and needed to be removed from it to increase the quality of the dataset.\n",
    "3- Imbalanced Data: Imbalanced classification refers to a classification predictive modeling problem where the number of examples in the training dataset for each class label is not balanced.\n",
    "4- Missing Values: In every dataset, there could be missing values at any points.\n",
    "\n",
    "In order to handle with this problem, there are two common method: \n",
    "1- Eliminating the missing value.\n",
    "2- Filling it with mean or medium.\n",
    "\n",
    "Outlier Detection: Outliers are data points that do not belong to a certain population.\n",
    "\n",
    "It is an abnormal observation that lies far away from other values. Briefly, an outlier is an observation that diverges from otherwise well-structured data.\n",
    "There are five common methods to deal with it:\n",
    "1-Standard Deviation: If there are any data points that are more than 3 times the standard deviation, then those points are very likely to be anomalous or outliers.\n",
    "\n",
    "2-Boxplots: Box plots are a graphical depiction of numerical data through their quantiles. Any data points that show above or below the whiskers, can be considered outliers or anomalous.\n",
    "3- DBScan Clustering: DBScan is a clustering algorithm used cluster data into groups. It is also used as a density-based anomaly detection method with either single or multi-dimensional data.\n",
    "It is a more complicated method than standard deviation and boxplots.\n",
    "\n",
    "4- Isolation Forest: Isolation Forest is an unsupervised learning algorithm that belongs to the ensemble decision trees family. It is a different method from others.\n",
    "It explicitly isolates anomalies instead of profiling and constructing normal points and regions by assigning a score to each data point.\n",
    "\n",
    "5- Robust Random Cut Forest: Random Cut Forest (RCF) algorithm is Amazon’s unsupervised algorithm for detecting anomalies.\n",
    "It works by associating an anomaly score as well. Low score values indicate that the data point is considered as normal. High values indicate the presence of an anomaly in the data.\n",
    "\n",
    "Feature Scaling: There are two types of scaling the features: \n",
    "1- Standardization \n",
    "2- Normalization.\n",
    "\n",
    "Standardization: The new value is defined as dependent on the standard deviation and the mode of the data.\n",
    "\n",
    "Normalization: The new value is dependent on:\n",
    "1- when the feature is more or less uniformly distributed across a fixed range: linear scaling -> maximum and minimum values of data inputs\n",
    "2- when the feature contains extreme outliers: clipping -> maximum and minimum values of data inputs\n",
    "3- when the feature conforms to the power law: log scaling -> logarithms of data inputs\n",
    "4- when the feature distribution does not contain extreme values: z-score -> the standard deviation and the mode of the data\n",
    "    \n",
    "Bucketing (Binning): It is an important method used to minimize the effects of noisy data.The actual data values are divided into small intervals known as bins and then they are replaced by a general value calculated for that bin.\n",
    "\n",
    "Feature Encoding: Some transformations can be performed on the data so that it can be easily accepted as input for machine learning algorithms.\n",
    "There are two common types of feature encoding:\n",
    "1- Nominal: Any one-to-one mapping can be done which retains the meaning. Example: One-hot encoding -> 001: Red 010: Yellow 100: Green for a traffic light encoding.\n",
    "2- Ordinal: An order-preserving change of values. Example: 0: less 1: equal 2: more\n",
    "    \n",
    "Train/Validation/Test Split: There are the explanation of types of dataset above. The split process is very important in data preparation.\n",
    "Available data refers to training dataset and test dataset while new available data refers to training dataset and validation dataset.\n",
    "It is more accurate to use validation dataset together with others. The percentage of training dataset should be more than the other types.\n",
    "\n",
    "Cross Validation: Testing a prediction function on the same data is a critical mistake. It is called overfitting.\n",
    "To avoid it, the most effective way is to hold out part of the available data as a test set.\n",
    "\n",
    "5)\n",
    "Discrete data involves round, concrete numbers that are determined by counting while continuous data involves complex numbers that are measured across a specific time interval.\n",
    "That can be said discrete variables are countable and finite but continuous variables are uncountable and infinite (take an infinite time to count).\n",
    "Example of discrete variable: The number of students in a school (as points)\n",
    "Example of continuous variable: The annual temperature values (as a distinct line)\n",
    "    \n",
    "6)\n",
    "This is a graph showing flower width on the horizontal axis and their proportions on the vertical axis.\n",
    "This graph consist of a curve (distinct line) so the type of data variable is continuous. Rises and drops are observed according to the rate of change of the distribution.\n",
    "\n",
    "Process:\n",
    "    1- Gathering Data  \n",
    "    2- Sorting into Balanced and Imbalanced Data  \n",
    "    3- Balancing the Imbalanced Data  \n",
    "    4- Fitting Missing Values with the Mean or Median \n",
    "    5- Cleaning Noise Data\n",
    "    6- Standardization and Normalization \n",
    "    7- Feature Extraction (PCA?) \n",
    "    8- Dataset Split \n",
    "    9- Ready\n",
    "\n",
    "\"\"\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
